{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Variational Laplace EM for SLDS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# +\nimport autograd.numpy as np\nimport autograd.numpy.random as npr\nnpr.seed(0)\n\nimport matplotlib.pyplot as plt\n# %matplotlib inline\n\nimport seaborn as sns\nsns.set_style(\"white\")\nsns.set_context(\"talk\")\n\ncolor_names = [\"windows blue\",\n               \"red\",\n               \"amber\",\n               \"faded green\",\n               \"dusty purple\",\n               \"orange\",\n               \"clay\",\n               \"pink\",\n               \"greyish\",\n               \"mint\",\n               \"light cyan\",\n               \"steel blue\",\n               \"forest green\",\n               \"pastel purple\",\n               \"salmon\",\n               \"dark brown\"]\n\ncolors = sns.xkcd_palette(color_names)\n\nimport ssm\nfrom ssm.util import random_rotation, find_permutation\n# -\n\n# Set the parameters of the SLDS\nT = 1000    # number of time bins\nK = 5       # number of discrete states\nD = 2       # number of latent dimensions\nN = 10      # number of observed dimensions\n\n# +\n# Make an SLDS with the true parameters\ntrue_slds = ssm.SLDS(N, K, D, emissions=\"gaussian_orthog\")\nfor k in range(K):\n    true_slds.dynamics.As[k] = .95 * random_rotation(D, theta=(k+1) * np.pi/20)\nz, x, y = true_slds.sample(T)\n\n# Mask off some data\nmask = npr.rand(T, N) < 0.9\ny_masked = y * mask\n# -\n\n# # Variational Laplace-EM\n#\n# We will fit a switching linear dynamical system (SLDS) to the observed data $y$. Our approach combines variational inference and a Laplace approximation. The objective we maximize is the evidence lower bound (ELBO)\n#\n# $$ \\mathcal{L}_q(\\theta) = \\mathbb{E}_{q(z) q(x)}[\\log p(z,x,y|\\theta)] - \\mathbb{E}_{q(z)}[\\log q(z)] - \\mathbb{E}_{q(x)}[\\log q(x)] $$\n#\n# where $\\theta$ are the model parameters, $z$ are the discrete latent variables, and $x$ are the continuous latent variables. We introduced an approximate posterior that factorizes over the discrete and continuous latent variables, $p(z, x \\, | \\, \\theta, y) \\approx q(z) \\, q(x)$. This form of the posterior corresponds to the `variational_posterior=\"structured_meanfield\"` argument in the `fit` method.\n#\n# The variational Laplace-EM inference method performs three updates at each iteration:\n# 1. Update $q(z)$ using the optimal coordinate ascent variational inference update\n# 2. Update $q(x)$ using a Laplace approximation at the most likely latent path (note that this step is not guaranteed to increase the ELBO)\n# 3. Update $\\theta$ by optimizing the ELBO with respect to the model parameters\n#\n# You can initialize a variational posterior and fit the model by running the function\n# ```\n# # Fit the model using Laplace-EM with a structured variational posterior\n# elbos, q_lem = slds.fit(data, inputs, masks, method=\"laplace_em\",\n#                         variational_posterior=\"structured_meanfield\",\n#                         initialize=False, num_iters=100)\n# ```\n# The output variables are the values of the objective at each iteration `elbos` and the variational posterior object `q_lem`. If you have already initialized a variational posterior object, you can pass that object as the argument of the `variational_posterior` parameter instead. Here, the `initialize` parameter was set to `False`, which assumes we have already initialized the model. Additionally, note that in `fit` you can also pass in keyword arguments for initialization of the variational posterior. \n\n# ### Hyperparameters\n# There are a number of hyperparameters that can be used when fitting with `laplace_em`. These include generic hyperparameters:\n# - `num_iters` - the number of iterations to run (default = 100)\n# - `learning` - optimize the model parameters when True (default = True)\n#     \n# Discrete state hyperparameters:\n# - `num_samples` - number of Monte Carlo samples (default = 1) used for evaluating expectations with respect to $q(x)$ in the update for $q(z)$\n#\n# Continuous state hyperparameters:\n# - `continuous_optimizer`, `continuous_tolerance`, `continuous_maxiter` specify parameters of the optimization for finding the most likely latent path in the continuous latent update. We recommend using the default optimizer Newton's method (`newton`) to compute the most likely latent path. However, Hessian-free optimization is supported with `lbfgs`. The tolerance and maxiter parameters can be adjusted for the user's requirements. \n#\n# Model parameter update hyperparameters:\n# - `alpha` - parameter in $[0,1)$ with default $0.5$ that determines how greedy we are in updating the model parameters at each iteration. This is only used in conjunction with `parameters_update=\"mstep\"`. \n# - `parameters_update` - the model parameter updates are implemented via an m-step given a single sample from $q(x)$ (default, `\"mstep\"`) or whether using SGD with samples from $q(x)$ (`\"sgd\"`).\n# - `emission_optimizer` - the optimizer used to update parameters in the m-step. This defaults to the `adam` optimizer when using the `\"sgd\"` parameter updates. \n# - `emission_optimizer_maxiter` - the maximum number of iterations for the inner loop of optimizing the emissions parameters. \n\n# ### Exploring the effect of the `alpha` hyperparameter\n#\n# In the `\"mstep\"` parameters update, we optimize the joint log-likelihood conditioned on a single sample from $q(x)$ to get a point estimate of the parameters $\\theta^\\star$. The new parameters at iteration $t$ are then given by\n#\n# $$ \\theta^{(t)} = (1 - \\alpha) \\, \\theta^\\star + \\alpha \\, \\theta^{(t-1)}. $$\n#\n# If `alpha=0.0` we set the parameters to $\\theta^\\star$, whereas for nonzero `alpha` the parameters are partially updated towards $\\theta^\\star$. The default is `alpha=0.5`. We have found that performance varies for different values of `alpha` and that the performance can depend on the specific problem. We will demonstrate below how `alpha` can affect inference by sweeping over five different values of `alpha` in model fits below.\n\nalphas = [0.0, 0.25, 0.5, 0.75, 0.9]\nresults = {}\nfor alpha in alphas:\n    print(\"Fitting SLDS with Laplace-EM, alpha = {}\".format(alpha))\n\n    # Create the model and initialize its parameters\n    slds = ssm.SLDS(N, K, D, emissions=\"gaussian_orthog\")\n    slds.initialize(y_masked, masks=mask)\n\n    # Fit the model using Laplace-EM with a structured variational posterior\n    q_lem_elbos, q_lem = slds.fit(y_masked, masks=mask, method=\"laplace_em\",\n                               variational_posterior=\"structured_meanfield\",\n                               initialize=False, num_iters=100, alpha=alpha)\n\n    # Get the posterior mean of the continuous states\n    q_lem_x = q_lem.mean_continuous_states[0]\n\n    # Find the permutation that matches the true and inferred states\n    slds.permute(find_permutation(z, slds.most_likely_states(q_lem_x, y)))\n    q_lem_z = slds.most_likely_states(q_lem_x, y)\n\n    # Smooth the data under the variational posterior\n    q_lem_y = slds.smooth(q_lem_x, y)\n\n    results[(alpha)] = (slds, q_lem, q_lem_elbos, q_lem_x, q_lem_z, q_lem_y)\n\n# Plot the ELBOs\nq_elbos = []\nfor alpha in alphas:\n    _,_,q_lem_elbos,_,_,_ = results[(alpha)]\n    q_elbos += [q_lem_elbos]\n    plt.plot(q_lem_elbos[1:], label=\"alpha = {}\".format(alpha), alpha=0.8)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"ELBO\")\nq_max = np.array(q_elbos).max()\nplt.ylim([q_max-0.1*np.abs(q_max),q_max+0.01*np.abs(q_max)])\nplt.legend()\n\n# +\n# Plot the true and inferred states\nxlim = (0, 1000)\n\nzs = []\nfor alpha in alphas:\n    (_, _, _, _, q_lem_z, _) = results[(alpha)]\n    zs += [q_lem_z]\n    \nplt.figure(figsize=(16,4))\nplt.imshow(np.row_stack((z, zs)), aspect=\"auto\", interpolation=\"none\")\nplt.plot(xlim, [0.5, 0.5], '-k', lw=2)\nplt.yticks([0,1,2,3,4,5],[\"$z_{\\\\mathrm{true}}$\", *alphas])\nplt.ylabel(\"alpha\")\nplt.xlim(xlim)\n# -\n\nplt.figure(figsize=(8,4))\nfor d in range(D):\n    plt.plot(x[:,d] + 4 * d, '-', color='k', label=\"True\" if d==0 else None)\n    for i, alpha in enumerate(alphas):\n        _,_,_,q_lem_x,_,_ = results[(alpha)]\n        plt.plot(q_lem_x[:,d] + 4 * d, '-',  color=colors[i+1], label=alpha if d == 0 else None, alpha=0.75)\nplt.ylabel(\"$x$\")\nplt.xlim(xlim)\nplt.legend()\n\n# Plot the smoothed observations\nplt.figure(figsize=(12,8))\nfor n in range(N):\n    plt.plot(y[:, n] + 6 * n, '-', color='k', label=\"True\" if n == 0 else None)\n    for i, alpha in enumerate(alphas):\n        _,_,_,_,_,q_lem_y = results[(alpha)]\n        plt.plot(q_lem_y[:,n] + 6 * n, '-',  color=colors[i+1], label=alpha if n == 0 else None, alpha=0.75)\nplt.legend()\nplt.xlabel(\"time\")\nplt.xlim(xlim)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}