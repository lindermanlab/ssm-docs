{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Simple HMM Demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# + [markdown] nbpresent={\"id\": \"5918355f-c759-41e8-9cc9-64baf78695b3\"}\n# # Hidden Markov Model Demo\n\n# + [markdown] nbpresent={\"id\": \"2b6476b4-bceb-48bc-8957-e943d943c162\"}\n# A Hidden Markov Model (HMM) is one of the simpler graphical models available in _SSM_. This notebook demonstrates creating and sampling from and HMM using SSM, and fitting an HMM to synthetic data. A full treatment of HMMs is beyond the scope of this notebook, but there are many good resources. [Stanford's CS228 Lecture Notes](https://ermongroup.github.io/cs228-notes/) provide a good introduction to HMMs and other graphical models. [Pattern Recognition and Machine Learning](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf) by Christopher Bishop covers HMMs and how the EM algorithm is used to fit them from data.\n#   \n#   \n# The goal of these notebooks is to introduce state-space models to practitioners who have some familiarity with them, but who may not have used these models in practice before. As such, we've included a few exercises to try as you make your way through the notebooks.\n\n# + [markdown] nbpresent={\"id\": \"7aec52f3-b963-4afb-b2a4-444b30304575\"}\n# ## 1. Setup\n# The line `import ssm` imports the package for use. Here, we have also imported a few other packages for plotting.\n\n# + nbpresent={\"id\": \"346a61a3-9216-480d-b5b8-39a78782a8c3\"}\nimport autograd.numpy as np\nimport autograd.numpy.random as npr\nnpr.seed(0)\n\nimport ssm\nfrom ssm.util import find_permutation\nfrom ssm.plots import gradient_cmap, white_to_color_cmap\n\nimport matplotlib.pyplot as plt\n# %matplotlib inline\n\nimport seaborn as sns\nsns.set_style(\"white\")\nsns.set_context(\"talk\")\n\ncolor_names = [\n    \"windows blue\",\n    \"red\",\n    \"amber\",\n    \"faded green\",\n    \"dusty purple\",\n    \"orange\"\n    ]\n\ncolors = sns.xkcd_palette(color_names)\ncmap = gradient_cmap(colors)\n\n\n# Speficy whether or not to save figures\nsave_figures = True\n\n# + [markdown] nbpresent={\"id\": \"e6b9c054-f24c-4271-85b5-0a8e795dc333\"}\n# ## 2. Create an HMM\n# An HMM consists of a set of hidden state variable, $z$, which can take on one of $K$ values (for our purposes, HMMs will always have discrete states), along with a set of transition probabilities for how the hidden state evolves over time. \n# In other words, we have $z_t \\in \\{1, \\ldots, K\\}$, where $z_t = k$ denotes that the hidden variable is in state $k$ at time $t$.\n#\n#\n# The key assumption in an HMM is that only the most recent state affects the next state. In mathematical terms:\n#\n# $$\n# p(z_t \\mid z_{t-1}, z_{t-2}, \\ldots, z_1) = p(z_t \\mid z_{t-1})\n# $$\n#\n# In an HMM, we don't observe the state itself. Instead, we get a noisy observation of the state at each time step according to some observation model. We'll use $x_t$ to denote the observation at time step $t$. The observation can be a vector or scalar. We'll use $D$ to refer to the dimensionality of the observation. A few of the supported observation models are:\n#\n# 1. **Gaussian**: Each discrete state $z_t = k$ is associated with a $D$-dimensional mean $\\mu_k$ and covariance matrix $\\Sigma_k$. Each observation $z_t$ comes from a Gaussian distribution centered at the associated mean, with the corresponding covariance.\n#\n# 2. **Student's T**: Same as Gaussian, but the observations come from a Student's-T Distribution.\n#\n# 3. **Bernoulli**: Each element of the $D$-dimensional observation is a Bernoulli (binary) random variable. Each discrete state $Z_i$ determines the probability that each element in the observation is nonzero.\n#\n# _Note: SSM supports many other observation models for HMMs. We are in the process of creating full-standalone documentation to describe them. For now, the best way to learn about SSM's other functionality is look at the source code. The observation models are described in observations.py._\n#\n#\n# In the below example, we create an instance of the HMM with 5 discrete states and 2 dimensional observations. We store our HMM instance in a variable called true_hmm with this line:\n#\n# `\n# true_hmm = ssm.HMM(K, D, observations=\"gaussian\")\n# `\n#\n# We then manually set the means for each latent state to make them farther away (this makes them easier to visualize).\n#\n# `\n# true_hmm.observations.mus = 3 * np.column_stack((np.cos(thetas), np.sin(thetas)))\n# `\n#\n# Here we are modifying the `observations` instance associated with the HMM we created above. We could also change the covariance, but for now we're leaving it with the default (identity covariance).\n\n# + nbpresent={\"id\": \"564edd16-a99d-4329-8e31-98fe1e1cef79\"}\n# Set the parameters of the HMM\ntime_bins = 200   # number of time bins\nnum_states = 5    # number of discrete states\nobs_dim = 2       # dimensionality of observation\n\n# Make an HMM\ntrue_hmm = ssm.HMM(num_states, obs_dim, observations=\"gaussian\")\n\n# Manually tweak the means to make them farther apart\nthetas = np.linspace(0, 2 * np.pi, num_states, endpoint=False)\ntrue_hmm.observations.mus = 3 * np.column_stack((np.cos(thetas), np.sin(thetas)))\n\n# + [markdown] nbpresent={\"id\": \"846d39dd-47a8-4b70-860f-6943eb17fc7a\"}\n# ## 3. Sample from the HMM\n#\n# We draw samples from an HMM using the `sample` method:  \n# `true_states, obs = true_hmm.sample(time_bins)`.  \n#\n# This returns a tuple $(z, x)$ of the latent states and observations, respectively.\n# In this case, `true_states` will be an array of size $(200,)$ because it contains the discrete state $z_t$ across $200$ time-bins. `obs` will be an array of size $(200, 2)$ because it contains the observations across $200$ time bins, and each observation is two dimensional.\n# We have specified the number of time-steps by passing `time_bins` as the argument to the `sample` method.\n#\n# In the next line, we retrieve the log-likelihood of the data we observed:  \n# `true_ll = true_hmm.log_probability(obs)`  \n#\n# This tells us the relative probability of our observations. In the next section, when we fit an HMM to the data we generated, the true log-likelihood will be helpful for determining if our fitting algorithm succeeded.\n\n# + nbpresent={\"id\": \"c441ffc6-38cb-4933-97b2-f62897046fd6\"}\n# Sample some data from the HMM\ntrue_states, obs = true_hmm.sample(time_bins)\ntrue_ll = true_hmm.log_probability(obs)\n\n# + nbpresent={\"id\": \"c9b4a46a-2f86-4b7f-adb6-70c667a1ac67\"}\n# Plot the observation distributions\nlim = .85 * abs(obs).max()\nXX, YY = np.meshgrid(np.linspace(-lim, lim, 100), np.linspace(-lim, lim, 100))\ndata = np.column_stack((XX.ravel(), YY.ravel()))\ninput = np.zeros((data.shape[0], 0))\nmask = np.ones_like(data, dtype=bool)\ntag = None\nlls = true_hmm.observations.log_likelihoods(data, input, mask, tag)\n\n# + [markdown] nbpresent={\"id\": \"a201a5b1-0cff-4e1f-9367-c25a89ebac41\"}\n# Below, we plot the samples obtained from the HMM, color-coded according to the underlying state. The solid curves show regions of of equal probability density around each mean. The thin gray lines trace the latent variable as it transitions from one state to another.\n\n# + nbpresent={\"id\": \"0feabc13-812b-4d5e-ac24-f8327ecb4d27\"}\nplt.figure(figsize=(6, 6))\nfor k in range(num_states):\n    plt.contour(XX, YY, np.exp(lls[:,k]).reshape(XX.shape), cmap=white_to_color_cmap(colors[k]))\n    plt.plot(obs[true_states==k, 0], obs[true_states==k, 1], 'o', mfc=colors[k], mec='none', ms=4)\n    \nplt.plot(obs[:,0], obs[:,1], '-k', lw=1, alpha=.25)\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nplt.title(\"Observation Distributions\")\n\nif save_figures:\n    plt.savefig(\"hmm_1.pdf\")\n\n# + [markdown] nbpresent={\"id\": \"a58c7a02-2777-4af8-982f-e279bd3bbeb6\"}\n# Below, we visualize each component of of the observation variable as a time series. The colors correspond to the latent state. The dotted lines represent the \"true\" values of the observation variable (the mean) while the solid lines are the actual observations sampled from the HMM.\n\n# + nbpresent={\"id\": \"1ec5ac27-2d23-4660-8702-4156f8ffdf39\"}\n# Plot the data and the smoothed data\nlim = 1.05 * abs(obs).max()\nplt.figure(figsize=(8, 6))\nplt.imshow(true_states[None,:],\n           aspect=\"auto\",\n           cmap=cmap,\n           vmin=0,\n           vmax=len(colors)-1,\n           extent=(0, time_bins, -lim, (obs_dim)*lim))\n\nEy = true_hmm.observations.mus[true_states]\nfor d in range(obs_dim):\n    plt.plot(obs[:,d] + lim * d, '-k')\n    plt.plot(Ey[:,d] + lim * d, ':k')\n\nplt.xlim(0, time_bins)\nplt.xlabel(\"time\")\nplt.yticks(lim * np.arange(obs_dim), [\"$x_{}$\".format(d+1) for d in range(obs_dim)])\n\nplt.title(\"Simulated data from an HMM\")\n\nplt.tight_layout()\n\nif save_figures:\n    plt.savefig(\"hmm_2.pdf\")\n\n# + [markdown] nbpresent={\"id\": \"093b73b4-65a9-40ac-83ba-334a10736e01\"}\n# ### Exercise 3.1: Change the observation model\n# Try changing the observation model to Bernoulli and visualizing the sampled data. You'll need to create a new HMM object with Bernoulli observations. Then, use the `sample` method to sample from it. Visualizing the mean vectors and contours makes sense for Gaussian observations, but might not be the best way to visualize Bernoulli observations. \n\n# + nbpresent={\"id\": \"74a8cecc-d647-4921-bb7e-4037d89065ea\"}\n# Your code here: create an HMM with Bernoulli observations\n# ---------------------------------------------------------\n\n\n# + [markdown] nbpresent={\"id\": \"759699ce-fffa-4667-90af-267122e39f01\"}\n# # 4. Fit an HMM to synthetic data\n# This is all fine, but so far we haven't done anything that useful. It's far more interesting to learn an HMM from data. In the following cells, we'll use the synthetic data we generated above to fit an HMM from scratch. This is done in the following lines:\n#\n# `\n# hmm = ssm.HMM(num_states, obs_dim, observations=\"gaussian\")\n# hmm_lls = hmm.fit(obs, method=\"em\", num_em_iters=N_iters)\n# `\n#\n# In the first line, we create a new HMM instance called `hmm` with a gaussian observation model, as in the previous case. Because we haven't specified anything, the transition probabilities and observation means will be randomly initialized. In the next line, we use the `fit` method to learn the transition probabilities and observation means from data. We set the method to `em` (expectation maximization) and specify the maximum number of iterations which will be used to fit the data. The `fit` method returns a numpy array which shows the log-likelihood of the data over time. We then plot this and see that the EM algorithm quickly converges.\n\n# + nbpresent={\"id\": \"d9064e18-01ca-43d4-a866-1b796cc94297\"}\ndata = obs # Treat observations generated above as synthetic data.\nN_iters = 50\n\n## testing the constrained transitions class\nhmm = ssm.HMM(num_states, obs_dim, observations=\"gaussian\")\n\nhmm_lls = hmm.fit(obs, method=\"em\", num_iters=N_iters, init_method=\"kmeans\")\n\nplt.plot(hmm_lls, label=\"EM\")\nplt.plot([0, N_iters], true_ll * np.ones(2), ':k', label=\"True\")\nplt.xlabel(\"EM Iteration\")\nplt.ylabel(\"Log Probability\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# + [markdown] nbpresent={\"id\": \"f9335974-fcee-4a2e-827f-b22a12ed688f\"}\n# The below cell is a bit subtle. In the first section, we sampled from the HMM and stored the resulting latent state $z$ in a variable called `state`. \n# Now, we are treating our observations from the previous section as data, and seeing whether we can infer the true state given only the observations. However, there is no guarantee that the states we learn correspond to the original states from the true HMM. In order to account for this, we need to find a permutation of the states of our new HMM so that they align with the states of the true HMM from the prior section. This is done in the following two lines:\n#\n# `most_likely_states = hmm.most_likely_states(obs)\n# hmm.permute(find_permutation(true_states, most_likely_states))\n# `  \n#   \n# In the first line, we use the `most_likely_states` method to infer the most likely latent states given the observations.  In the second line we call the `find_permutation` function the permutation that best matches the true state. We then use the `permute` method on our `hmm` instance to permute its states accordingly.\n#\n#\n\n# + nbpresent={\"id\": \"35947156-e3a9-44d6-ab79-aea66d05cda7\"}\n# Find a permutation of the states that best matches the true and inferred states\nmost_likely_states = hmm.most_likely_states(obs)\nhmm.permute(find_permutation(true_states, most_likely_states))\n\n# + [markdown] nbpresent={\"id\": \"03d4efcd-66a8-4e0b-8558-6df4658382d4\"}\n# Below, we plot the inferred states ($z_{\\mathrm{inferred}}$) and the true states ($z_{\\mathrm{true}}$) over time. We see that the two match very closely, but not exactly. The model sometimes has difficulty inferring the state if we only observe that state for a very short time.\n\n# + nbpresent={\"id\": \"84b20c35-4187-4b2b-8287-c99212f17a4b\"}\n# Plot the true and inferred discrete states\nhmm_z = hmm.most_likely_states(data)\n\nplt.figure(figsize=(8, 4))\nplt.subplot(211)\nplt.imshow(true_states[None,:], aspect=\"auto\", cmap=cmap, vmin=0, vmax=len(colors)-1)\nplt.xlim(0, time_bins)\nplt.ylabel(\"$z_{\\\\mathrm{true}}$\")\nplt.yticks([])\n\nplt.subplot(212)\nplt.imshow(hmm_z[None,:], aspect=\"auto\", cmap=cmap, vmin=0, vmax=len(colors)-1)\nplt.xlim(0, time_bins)\nplt.ylabel(\"$z_{\\\\mathrm{inferred}}$\")\nplt.yticks([])\nplt.xlabel(\"time\")\n\nplt.tight_layout()\n\n# + [markdown] nbpresent={\"id\": \"75818ecd-323a-4cdf-a52a-3703b3e82123\"}\n# An HMM can also be used to smooth data (once its parameters are learned) but computing the mean observation under the posterior distribution of latent states. \n# Let's say, for example, that during time steps 0 to 10 the model estimates a 0.3 probability of being in state 1, and a 0.7 probability of being in state 2, given the observations $x$.\n# Mathematically, that's saying we've computed the following probabilities:  \n# $$\n# p(z=1 \\mid X) = 0.3\\\\\n# p(z=3 \\mid X) = 0.7\n# $$\n#   \n# The smoothed observations would then be $0.3 \\mu_1 + 0.7 \\mu_2$, where we $\\mu_i$ is the mean for the observations in state $i$.\n# In the cell below, we use `hmm.smooth(obs)` to smooth the data this way. The orange and blue lines show the smoothed data, and the black lines show the original noisy observations.\n\n# + nbpresent={\"id\": \"69dc9764-e7bc-4ab5-80a3-ba107e323531\"}\n# Use the HMM to \"smooth\" the data\nhmm_x = hmm.smooth(obs)\n\nplt.figure(figsize=(8, 4))\nplt.plot(obs + 3 * np.arange(obs_dim), '-k', lw=2)\nplt.plot(hmm_x + 3 * np.arange(obs_dim), '-', lw=2)\nplt.xlim(0, time_bins)\nplt.ylabel(\"$x$\")\n# plt.yticks([])\nplt.xlabel(\"time\")\n\n# + [markdown] nbpresent={\"id\": \"747730ff-ab9a-4aff-9da4-6e7b203d2aa6\"}\n# ### 4.1. Visualize the Transition Matrices\n# The dynamics of the hidden state in an HMM are specified by the transition probabilities $p(z_t \\mid z_{t-1})$. It's standard to pack these probabilities into a stochastic matrix $A$ where $A_{ij} = p(z_t = j \\mid z_{t-1} = i)$.\n#\n# In SSM, we can access the transition matrices using `hmm.transitions.transition` matrix. In the following two lines, we retrives the transition matrices for the true HMM, as well as the HMM we learned from the data, and compare them visually.\n\n# + nbpresent={\"id\": \"67124d1b-c672-47a1-92cc-5538012bcd48\"}\ntrue_transition_mat = true_hmm.transitions.transition_matrix\nlearned_transition_mat = hmm.transitions.transition_matrix\n\nfig = plt.figure(figsize=(8, 4))\nplt.subplot(121)\nim = plt.imshow(true_transition_mat, cmap='gray')\nplt.title(\"True Transition Matrix\")\n\nplt.subplot(122)\nim = plt.imshow(learned_transition_mat, cmap='gray')\nplt.title(\"Learned Transition Matrix\")\n\ncbar_ax = fig.add_axes([0.95, 0.15, 0.05, 0.7])\nfig.colorbar(im, cax=cbar_ax)\nplt.show()\n\n\n# + [markdown] nbpresent={\"id\": \"e358a229-5f00-4a6c-9b13-011d2afff30c\"}\n# ### Excercise 4.2: Distribution of State Durations\n# Derive the theoretical distribution over state durations. Do the state durations we observe ($Z_{true}$ in section 4) match the theory? If you're stuck, imagine that the system starts in state $1$, i.e $z_1 = 1$. What's the probability that $z_2 = 1$? From here, you might be able to work forwards in time.\n#\n# When done, check if your derivation matches what we find in the section below.\n\n# + [markdown] nbpresent={\"id\": \"2a96744b-592a-4642-904f-27793f67d790\"}\n# ### 4.3: Visualize State Durations\n#\n\n# + nbpresent={\"id\": \"30e94251-7e72-42f6-9329-7f43500f5e05\"}\ntrue_state_list, true_durations = ssm.util.rle(true_states)\ninferred_state_list, inferred_durations = ssm.util.rle(hmm_z)\n\n# Rearrange the lists of durations to be a nested list where\n# the nth inner list is a list of durations for state n\ntrue_durs_stacked = []\ninf_durs_stacked = []\nfor s in range(num_states):\n    true_durs_stacked.append(true_durations[true_state_list == s])\n    inf_durs_stacked.append(inferred_durations[inferred_state_list == s])\n    \nfig = plt.figure(figsize=(8, 4))\nplt.hist(true_durs_stacked, label=['state ' + str(s) for s in range(num_states)])\nplt.xlabel('Duration')\nplt.ylabel('Frequency')\nplt.legend()\nplt.title('Histogram of True State Durations')\n\nfig = plt.figure(figsize=(8, 4))\nplt.hist(inf_durs_stacked, label=['state ' + str(s) for s in range(num_states)])\nplt.xlabel('Duration')\nplt.ylabel('Frequency')\nplt.legend()\nplt.title('Histogram of Inferred State Durations')\n\nplt.show()\n\n# + [markdown] nbpresent={\"id\": \"a36b24e0-89ce-401a-af4e-0303955ab0be\"}\n# ### Excercise 4.4: Fit an HMM using more data\n# We see that the above histograms do not match each other as closely as we might expect. They also don't match the theoretical distriubtion of durations all that closely (see Exercise 4.2). Part of the reason for this is that we have sampled from a relatively small number of time steps.   \n#\n# Try modifying the `time_bins` variable to sample for more time-steps (say 2000 or so).\n# Then, re-run the analysis above. Because of the larger time frame, some of the plots above may become hard to read, but the histogram of durations should more closely match what we expect.\n\n# + [markdown] nbpresent={\"id\": \"d93612d4-88a5-4c39-8d8d-b1ec4865ab70\"}\n# ### Exercise 4.5: Mismatched Observations\n# Imagine a scenario where the true data comes from an HMM with Student's T observations, but you fit an HMM with Gaussian observations. What might you expect to happen?  \n#\n# You can try simulating this: modify the code in Section 2 so that we create and HMM with Student's T observations. Then re-run the cells in Section 4, which will fit an HMM with Gaussian observations to the observed data. What do you see?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}