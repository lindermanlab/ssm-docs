{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Switching Linear Dynamical System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# ## Switching Linear Dynamical System Demo\n\n# _Switching Linear Dynamical Systems_ (SLDS) provide a natural way of combining Linear Dynamical Systems with Hidden Markov Models. They allow us to approximate a system that has globally non-linear dynamics by a series linear systems. A good reference for these types of systems is [\"Variational Inference for State Space models\"](https://www.cs.toronto.edu/~hinton/absps/switch.pdf) by Ghahramani and Hinton. \n#\n# An LDS comprises $K$ discrete hidden states, which evolve according to a Markov chain. We'll call the hidden state $z$, and use the notation $z_t = k$ to mean that the system is in state $k$ at time $t$. The Markov chain for the hidden state is specified by a state-transition matrix $Q$, where $Q_{ij} = P(z_t = j \\mid z_{t-1} = i)$.\n#\n# ### Generative Model for SLDS\n# The generative model for an SLDS combines an HMM with a set of linear dynamical systems as follows. In addition to the discrete state, we have a continuous latent state $x_t \\in \\mathbb{R}^D$ and an observation $y_t \\in \\mathbb{R}^N$. Each discrete state $\\{1,\\ldots,K\n# \\}$ is associated with a different dynamics matrix $A_k$ and a different measurement matrix $C_k$. Formally, we generate data from an SLDS as follows:\n#\n# 1. **Discrete State Update**. At each time step, sample a new discrete state $z_t \\mid z_{t-1}$ with probabilities given by a Markov chain.\n#\n# 2. **Continuous State Update**. Update the state using the dynamics matrix corresponding to the new discrete state:\n# $$\n# x_t = A_k x_{t-1} + V_k u_{t} + b_k + w_t\n# $$\n# $A_k$ is the dynamics matrix corresponding to discrete state $k$. $u_t$ is the input vector (specified by the user, not inferred by SSM) and $V_k$ is the corresponding control matrix. The vector $b$ is an offset vector, which can drive the dynamics in a particular direction. \n# The terms $w_t$ is a noise terms, which perturbs the dynamics. \n# Most commonly these are modeled as zero-mean multivariate Gaussians,\n# but one nice feature of SSM is that it supports many distributions for these noise terms. See the Linear Dynamical Systems notebook for a list of supported dynamics models.\n#\n# 3. **Emission**. We now make an observation of the state, according to the specified observation model. In the general case, the state controlls the observation via a Generalized Linear Model:\n# $$\n# y_t \\sim \\mathcal{P}(\\eta(C_k x_t + d_k + F_k u_t + v_t))\n# $$\n# $\\mathcal{P}$ is a probabibility distribution. The inner arguments form an affine measurement of the state, which is then passed through the inverse link function $\\eta(\\cdot)$.\n# In this case, $C_k$ is the measurement matrix corresponding to discrete state $k$, $d_k$ is an offset or bias term corresponding to discrete state $k$, $F_k$ is called the feedthrough matrix or passthrough matrix (it passes the input directly to the emission). In the Gaussian case, the emission can simply be written as $y_t = C_k x_t + d_k + F_k u_t + v_t$ where $v_t$ is a Gaussian r.v. See the Linear Dynamical System notebook for a list of the observation models supported by SSM.\n#   \n#\n#\n\n# ## 1. Setup\n# We import SSM as well as a few other utilities for plotting.\n\n# +\nimport autograd.numpy as np\nimport autograd.numpy.random as npr\nnpr.seed(0)\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n# %matplotlib inline\n\nimport seaborn as sns\n\nsns.set_style(\"white\")\nsns.set_context(\"talk\")\n\ncolor_names = [\"windows blue\",\n               \"red\",\n               \"amber\",\n               \"faded green\",\n               \"dusty purple\",\n               \"orange\",\n               \"clay\",\n               \"pink\",\n               \"greyish\",\n               \"mint\",\n               \"cyan\",\n               \"steel blue\",\n               \"forest green\",\n               \"pastel purple\",\n               \"salmon\",\n               \"dark brown\"]\n\ncolors = sns.xkcd_palette(color_names)\ncmap = ListedColormap(colors)\n\nimport ssm\nfrom ssm.util import random_rotation, find_permutation\nfrom ssm.plots import plot_dynamics_2d\n\nsave_figures = False\n# -\n\n# ## 2. Creating an SLDS and Sampling\n# Below, we set some parameters for our SLDS: 5 discrete states, latent state of dimension 2, emissions of dimensions 10. We'll be sampling for 100 time bins for the purpose of visualizing the output of our SLDS.\n#\n# We then create an SLDS object:\n# ```python\n# true_slds = ssm.SLDS(emissions_dim,\n#                      n_disc_states,\n#                      latent_dim,\n#                      emissions=\"gaussian_orthog\")\n# ```\n# We specify the emissions model as `\"gaussian_orthog\"` which ensures that each measurement matrix $C_k$ will be orthogonal. Because an orthogonal matrix is full-rank, this means that our system is fully observable. In other words, the emissions model does not \"losing\" information about the state.\n#\n# The syntax for sampling from an SLDS is the same as for an LDS:\n# ```python\n# states_z, states_x, emissions = true_lds.sample(time_bins)\n# ```\n# The sample function for SLDS returns a tuple of (discrete states, continuous states, observations).\n#\n\n# Set the parameters of the SLDS\ntime_bins = 100    # number of time bins\nn_disc_states = 5       # number of discrete states\nlatent_dim = 2       # number of latent dimensions\nemissions_dim = 10      # number of observed dimensions\n\n# +\n# Make an SLDS with the true parameters\ntrue_slds = ssm.SLDS(emissions_dim,\n                     n_disc_states,\n                     latent_dim,\n                     emissions=\"gaussian_orthog\")\n\nfor k in range(n_disc_states):\n    true_slds.dynamics.As[k] = .95 * random_rotation(latent_dim, theta=(k+1) * np.pi/20)\n    \nstates_z, states_x, emissions = true_slds.sample(time_bins)\n# -\n\n# ### 2.1 Visualize the Latent States\n# Below, we visualize the 2-dimensional trajectory of the continuous latent state $x_t$. The different colors correspond to different values of the discrete state variable $z_t$. We can see how the different colors correspond to different dynamics on the latent state.\n\n# +\nfor k in range(n_disc_states):\n    curr_states = states_x[states_z == k]\n    plt.plot(curr_states[:,0],\n             curr_states[:,1],\n             '-',\n             color=colors[k],\n             lw=3,\n             label=\"$z=%i$\" % k)\n    \n    # Draw lines connecting the latent state between discrete state transitions,\n    # so they don't show up as broken lines.\n    next_states = states_x[states_z == k+1]\n    if len(next_states) > 0 and len(curr_states) > 0:\n        plt.plot((curr_states[-1,0], next_states[0,0]),\n                 (curr_states[-1,1], next_states[0,1]),\n                 '-',\n                 color='gray',\n                 lw=1)\n\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nplt.title(\"Simulated Latent States\")\nplt.legend(bbox_to_anchor=(1.0,1.0))\nplt.show()\n\nplt.figure(figsize=(10,2))\ncmap_limited = ListedColormap(colors[0:n_disc_states])\nplt.imshow(states_z[None,:],  aspect=\"auto\", cmap=cmap_limited)\nplt.title(\"Simulated Discrete Latent States\")\nplt.yticks([])\nplt.xlabel(\"Time\")\nplt.show()\n# -\n\n# ### 2.1 Visualize the Emissions\n# Below, we visualize the 10-dimensional emissions from our SLDS.\n\n# +\nplt.figure(figsize=(10, 6))\ngs = plt.GridSpec(2, 1, height_ratios=(1, emissions_dim/latent_dim))\n\n# Plot the continuous latent states\nlim = abs(states_x).max()\nplt.subplot(gs[0])\nfor d in range(latent_dim):\n    plt.plot(states_x[:, d] + lim * d, '-k')\nplt.yticks(np.arange(latent_dim) * lim, [\"$x_{}$\".format(d+1) for d in range(latent_dim)])\nplt.xticks([])\nplt.xlim(0, time_bins)\nplt.title(\"Simulated Latent States\")\n\nlim = abs(emissions).max()\nplt.subplot(gs[1])\nfor n in range(emissions_dim):\n    plt.plot(emissions[:, n] - lim * n, '-')\nplt.yticks(-np.arange(emissions_dim) * lim, [\"$y_{{ {} }}$\".format(n+1) for n in range(emissions_dim)])\nplt.xlabel(\"time\")\nplt.xlim(0, time_bins)\n\nplt.title(\"Simulated emissions\")\nplt.tight_layout()\n\nif save_figures:\n    plt.savefig(\"lds_2.pdf\")\n# -\n\n# ## 3. Fit an SLDS From Data\n# SSM provides the capability to learn the parameters of an SLDS from data. In the above cells, we sampled from 100 time-steps in order to visualize the state trajectory.\n#\n# In order to learn an SLDS, we'll need more data, so we start by sampling for a longer period. In the following cells, we'll treat our observations as a dataset, and demonstrate how to learn an SLDS using SSM.\n\n# Sample again, for more time-bins\ntime_bins = 1000\nstates_z, states_x, emissions = true_slds.sample(time_bins)\ndata = emissions\n\n# ### 3.1 Compare Fitting Methods\n#\n# **Important Note:**  \n#  <span style=\"font-size:larger;\">\n# Understanding the following section is not necessary to use SSM! _For practical purposes, it is almost always best to use the Laplace-EM method, which is the default._\n# </span>\n#\n# **Parameter Learning for SLDS**  \n# Parameter learning in an SLDS requires approximate methods. SSM provides two approximate inference algorithms: Stochastic Variational Inference (`\"svi\"`), Laplace-Approximate EM (`\"laplace_em\"`). We don't have the space to describe these methods in detail here, but Stochastic Variational Inference was described in [\"Stochastic Variational Inference\"](http://www.columbia.edu/~jwp2128/Papers/HoffmanBleiWangPaisley2013.pdf) by Hoffamn et al. The Laplace Approximation is described in several sources, but a good reference for the context of state-space models is [\"Estimating State and Parameters in state-space models of Spike Trains,\"](https://pdfs.semanticscholar.org/a71e/bf112cabd47cc67284dc8c12ab7644195d60.pdf) a book chapter by Macke et al.\n#\n#\n#\n# **Approximate Posterior Distributions**\n# When using approximate methods, we must choose the form of the distribution we use to approximate the posterior. Here, SSM provides three options:\n# 1. `variational_posterior=\"meanfield\"`\n# The mean-field approximation uses a factorized distribution as the approximating posterior. Compatible with the SVI method.\n#\n# 2. `variational_posterior=\"tridiag\"`\n# This approximates the posterior using a Gaussian with a block tridiagonal covariance matrix, which can be thought of as approximating the SLDS posterior with the posterior from an LDS. Compatible with the SVI method.\n#\n# 3. `variational_posterior=\"structured_meanfield\"`\n# This assumes a posterior where the join distribution over the continuous and discrete latent states factors as follows. If $q(z,x \\mid y)$ is the joint posterior of the discrete and continuous states given the data, we use the approximation $q(z,x \\mid y) \\approx q(z \\mid y)q(x \\mid y)$, where $q(z \\mid y)$ is the posterior for a Markov chain. Compatible with the SVI and Laplace-EM methods.\n#\n# **Calling the Fit function in SSM**  \n# All models in SSM share the same general syntax for fitting a model from data. Below, we call the fit function using three different methods and compare convergence. The syntax is as follows:\n# ```python\n# elbos, posterior = slds.fit(data, method= \"...\",\n#                             variational_posterior=\"...\",\n#                             num_iters= ...)\n# ```\n# In the the call to `fit`, method should be one of {`\"svi\"`, `\"laplace_em\"`}.  \n# The `variational_posterior` argument should be one of {`\"mf\"`, `\"structured_meanfield\"`}. However, when using Laplace-EM _only_ structured mean field is supported.\n# Below, we fit using four methods, and compare convergence.\n\n# **Fit using BBVI and Mean-Field Posterior**\n\n# +\nprint(\"Fitting SLDS with BBVI and Mean-Field Posterior\")\n\n# Create the model and initialize its parameters\nslds = ssm.SLDS(emissions_dim, n_disc_states, latent_dim, emissions=\"gaussian_orthog\")\n\n# Fit the model using BBVI with a mean field variational posterior\nq_mf_elbos, q_mf = slds.fit(data, method=\"bbvi\",\n                            variational_posterior=\"mf\",\n                            num_iters=1000)\n\n# Get the posterior mean of the continuous states\nq_mf_x = q_mf.mean[0]\n\n# Find the permutation that matches the true and inferred states\nslds.permute(find_permutation(states_z, slds.most_likely_states(q_mf_x, data)))\nq_mf_z = slds.most_likely_states(q_mf_x, data)\n\n# Smooth the data under the variational posterior\nq_mf_y = slds.smooth(q_mf_x, data)\n# -\n\n# **Fit using BBVI and Structured Variational Posterior**\n\n# +\nprint(\"Fitting SLDS with BBVI using structured variational posterior\")\nslds = ssm.SLDS(emissions_dim, n_disc_states, latent_dim, emissions=\"gaussian_orthog\")\n\n# Fit the model using SVI with a structured variational posterior\nq_struct_elbos, q_struct = slds.fit(data, method=\"bbvi\",\n                               variational_posterior=\"tridiag\",\n                               num_iters = 1000)\n\n# Get the posterior mean of the continuous states\nq_struct_x = q_struct.mean[0]\n\n# Find the permutation that matches the true and inferred states\nslds.permute(find_permutation(states_z, slds.most_likely_states(q_struct_x, data)))\nq_struct_z = slds.most_likely_states(q_struct_x, data)\n\n# Smooth the data under the variational posterior\nq_struct_y = slds.smooth(q_struct_x, data)\n# -\n\n# **Fit using Laplace-EM**\n\n# +\nprint(\"Fitting SLDS with Laplace-EM\")\n\n# Create the model and initialize its parameters\nslds = ssm.SLDS(emissions_dim, n_disc_states, latent_dim, emissions=\"gaussian_orthog\")\n\n# Fit the model using Laplace-EM with a structured variational posterior\nq_lem_elbos, q_lem = slds.fit(data, method=\"laplace_em\",\n                               variational_posterior=\"structured_meanfield\",\n                               num_iters=100, alpha=0.0)\n\n# Get the posterior mean of the continuous states\nq_lem_x = q_lem.mean_continuous_states[0]\n\n# Find the permutation that matches the true and inferred states\nslds.permute(find_permutation(states_z, slds.most_likely_states(q_lem_x, data)))\nq_lem_z = slds.most_likely_states(q_lem_x, data)\n\n# Smooth the data under the variational posterior\nq_lem_y = slds.smooth(q_lem_x, data)\n# -\n\n# Plot the ELBOs\nplt.plot(q_mf_elbos, label=\"SVI: Mean-Field Posterior\")\nplt.plot(q_struct_elbos, label=\"SVI: Block-Tridiagonal Structured Posterior\")\nplt.plot(q_lem_elbos, label=\"Laplace-EM: Structured Mean-Field Posterior\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"ELBO\")\nplt.legend(bbox_to_anchor=(1.0,1.0))\nplt.title(\"Convergence for learning an SLDS\")\nplt.show()\n\n# ### 3.2 Exercise: The Evidence Lower Bound (ELBO)\n# In the SLDS model (and even in the LDS case with non-Gaussian observations), we can't optimize the log-likelihood directly. Instead, we optimize a lower bound on the log likelihood called the Evidence Lower Bound (ELBO). \n#\n# We denote the parameters of the model as $\\Theta$, which are considered fixed for the purposes of this exercise. Concretely, we need to find a lower bound on $\\log(P(Y \\mid \\Theta))$ where $Y=[y_1,\\ldots,y_T]$. Can you use Jensen's inequality to derive a lower bound on this likelihood?\n\n# ## 4. Visualize True and Inferred Latent States\n# Below, we compare how well each fitting algorithm recovers the discrete latent states. We then inspect the true vs. inferred continuos latent states.\n\n# +\n# Plot the true and inferred states\ntitles = [\"True\", \"Laplace-EM\", \"SVI with Structured MF\", \"SVI with MF\"]\nstates_list = [states_z, q_lem_z, q_struct_z, q_mf_z]\nfig, axs = plt.subplots(4,1, figsize=(8,6))\nfor (i, ax, states) in zip(range(len(axs)), axs, states_list):\n    ax.imshow(states[None,:], aspect=\"auto\", cmap=cmap_limited)\n    ax.set_yticks([])\n    ax.set_title(titles[i])\n    if i < (len(axs) - 1):\n        ax.set_xticks([])\n\nplt.suptitle(\"True and Inferred States for Different Fitting Methods\", va=\"baseline\")\nplt.tight_layout()\n\n# +\ntitle_str = [\"$x_1$\", \"$x_2$\"]\nfig, axs = plt.subplots(2,1, figsize=(14,4))\nfor (d, ax) in enumerate(axs):\n    ax.plot(states_x[:,d] + 4 * d, '-', color=colors[0], label=\"True\" if d==0 else None)\n    ax.plot(q_lem_x[:,d] + 4 * d, '-', color=colors[2], label=\"Laplace-EM\" if d==0 else None)\n    ax.set_yticks([])\n    ax.set_title(title_str[d], loc=\"left\", y=0.5, x=-0.03)\naxs[0].set_xticks([])\naxs[0].legend(loc=\"upper right\")\n\nplt.suptitle(\"True and Inferred Continuous States\", va=\"bottom\")\nplt.tight_layout()\n# -\n\n# ### 4.2 Exercise: Fitting with fewer datapoints\n# From the above plots, it seems we were able to match the discrete states quite well using our learned model. Try reducing the number of time-bins used for fitting from 1000 to 500 or 100. At what point do we begin to fit badly?\n\n# ## 5. Inference on unseen data\n# After learning a model from data, a common use-case is to compute the distribution over latent states given some new observations. For example, in the case of a simple LDS, we could use the Kalman Smoother to estimate the latent state trajectory given a set of observations. \n#\n# In the case of an SLDS (or Recurrent SLDS), the posterior over latent states can't be computed exactly. Instead, we need to live with a variational approximation to the true posterior. SSM allows us to compute this approximation using the `SLDS.approximate_posterior()` method. \n#\n# In the below example, we generate some new data from the true model. We then use the `approximate_posterior()` function to estimate the continuous and discrete states. \n\n# +\n# Generate data which was not used for fitting\ntime_bins = 100\ndata_z, data_x, data = true_slds.sample(time_bins)\n\n# Compute the approximate posterior over latent and continuous\n# states for the new data under the current model parameters.\nelbos, posterior = slds.approximate_posterior(data,\n                                              method=\"laplace_em\",\n                                              variational_posterior=\"structured_meanfield\",\n                                              num_iters=50)\n\n# Verify that the ELBO increases during fitting. We don't expect a substantial increase:\n# we are updating the estimate of the latent states but we are not changing model params.\nplt.plot(elbos)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"ELBO\")\nplt.show()\n# -\n\n# **Estimating Latent States**  \n#   \n# `posterior` is now an `ssm.variational.SLDSStructuredMeanFieldVariationalPosterior` object. Using this object, we can estimate the continuous and discrete states just like we did after calling the fit function.\n#\n# In the below cell, we get the estimated continuous states as follows:\n# ```python\n# posterior_x = posterior.mean_continuous_states[0]\n# ```\n# This line uses the `mean_continuous_states` property of the posterior object, which returns a list, where each entry of the list corresponds to a single trial of data. Since we have only passed in a single trial the list will have length 1, and we take the first entry.\n#\n# We then permute the discrete and continuous states to best match the ground truth. This is for aesthetic purposes when plotting. The following lines compute the best permutation which match the predicted states (`most_likely`) to the ground truth discrete states (`data_z`). We then permute the states of the SLDS accordingly:\n# ```python\n#\n# most_likely = slds.most_likely_states(posterior_x, data)\n# perm = find_permutation(data_z, most_likely)\n# slds.permute(perm)\n# z_est = slds.most_likely_states(posterior_x, data)\n#\n# ```\n#\n#\n\n# +\n# Get the posterior mean of the continuous states\nposterior_x = posterior.mean_continuous_states[0]\n\n# Find the permutation that matches the true and inferred states\nmost_likely = slds.most_likely_states(posterior_x, data)\nperm = find_permutation(data_z, most_likely)\nslds.permute(perm)\nz_est = slds.most_likely_states(posterior_x, data)\n\n# +\n# Plot the true and inferred states\ntitles = [\"True\", \"Estimated\"]\nstates_list = [data_z, z_est]\nfig, axs = plt.subplots(2,1, figsize=(6,4))\nfor (i, ax, states) in zip(range(len(axs)), axs, states_list):\n    ax.imshow(states[None,:], aspect=\"auto\", cmap=cmap_limited)\n    ax.set_yticks([])\n    ax.set_title(titles[i])\n    if i < (len(axs) - 1):\n        ax.set_xticks([])\n\nplt.suptitle(\"True and Inferred States using Structured Meanfield Posterior\", va=\"baseline\")\nplt.tight_layout()\n# -\n\nx_est = posterior.mean_continuous_states[0]\n\n# +\ntitle_str = [\"$x_1$\", \"$x_2$\"]\nfig, axs = plt.subplots(2,1, figsize=(14,4))\nfor (d, ax) in enumerate(axs):\n    ax.plot(data_x[:,d] + 4 * d, '-', color=colors[0], label=\"True\" if d==0 else None)\n    ax.plot(x_est[:,d] + 4 * d, '-', color=colors[2], label=\"Laplace-EM\" if d==0 else None)\n    ax.set_yticks([])\n    ax.set_title(title_str[d], loc=\"left\", y=0.5, x=-0.03)\naxs[0].set_xticks([])\naxs[0].legend(loc=\"upper right\")\n\nplt.suptitle(\"True and Estimated Continuous States\", va=\"bottom\")\nplt.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}