{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# HMM State Clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# + [markdown] nbpresent={\"id\": \"5918355f-c759-41e8-9cc9-64baf78695b3\"}\n# # HMM State Clustering\n# In this notebook we'll explore a post-hoc method for clustering HMM states.\n# The idea is, given an HMM which has been fit to data, we reduce the number of states hierarchically by merging pairs of states. Let's say we start with an HMM with K states. The idea is that we'll try merging every pair of states and see which merge makes the log-likelihood of the data go down the least. Once we find that pair, we have K-1 states, and we repeat this process until we have satisfactorily few states.\n#\n# **Note**: This notebook is a little rough around the edges.\n\n# + nbpresent={\"id\": \"346a61a3-9216-480d-b5b8-39a78782a8c3\"}\nimport autograd.numpy as np\nimport autograd.numpy.random as npr\nnpr.seed(0)\n\nimport ssm\nfrom ssm.util import find_permutation\nfrom ssm.plots import gradient_cmap, white_to_color_cmap\n\nimport matplotlib.pyplot as plt\n# %matplotlib inline\n\nimport seaborn as sns\nsns.set_style(\"white\")\nsns.set_context(\"talk\")\n\ncolor_names = [\n    \"windows blue\",\n    \"red\",\n    \"amber\",\n    \"faded green\",\n    \"dusty purple\",\n    \"orange\"\n    ]\n\ncolors = sns.xkcd_palette(color_names)\ncmap = gradient_cmap(colors)\n\n# + nbpresent={\"id\": \"564edd16-a99d-4329-8e31-98fe1e1cef79\"}\n# Set the parameters of the HMM\ntime_bins = 1000   # number of time bins\nnum_states = 6   # number of discrete states\nobs_dim = 2       # dimensionality of observation\n\n# Make an HMM\ntrue_hmm = ssm.HMM(num_states, obs_dim, observations=\"gaussian\")\n\n# Manually tweak the means to make them farther apart\nthetas = 2 * np.pi * npr.rand(num_states)\ntrue_hmm.observations.mus = 3 * np.column_stack((np.cos(thetas), np.sin(thetas)))\n# -\n\n# ## For demonstration, make the last two states very similar\n\ntrue_hmm.observations.mus[-1] = true_hmm.observations.mus[-2] + 1e-3 * npr.randn(obs_dim) \n\n# + [markdown] nbpresent={\"id\": \"846d39dd-47a8-4b70-860f-6943eb17fc7a\"}\n# ## Sample some synthetic data\n\n# + nbpresent={\"id\": \"c441ffc6-38cb-4933-97b2-f62897046fd6\"}\ntrue_states, data = true_hmm.sample(time_bins)\ntrue_ll = true_hmm.log_probability(data)\n\n# + nbpresent={\"id\": \"c9b4a46a-2f86-4b7f-adb6-70c667a1ac67\"}\n# Plot the observation distributions\nlim = .85 * abs(data).max()\nXX, YY = np.meshgrid(np.linspace(-lim, lim, 100), np.linspace(-lim, lim, 100))\ngrid = np.column_stack((XX.ravel(), YY.ravel()))\ninput = np.zeros((data.shape[0], 0))\nmask = np.ones_like(grid, dtype=bool)\ntag = None\nlls = true_hmm.observations.log_likelihoods(grid, input, mask, tag)\n\n# + [markdown] nbpresent={\"id\": \"a201a5b1-0cff-4e1f-9367-c25a89ebac41\"}\n# Below, we plot the samples obtained from the HMM, color-coded according to the underlying state. The solid curves show regions of of equal probability density around each mean. The thin gray lines trace the latent variable as it transitions from one state to another.\n\n# + nbpresent={\"id\": \"0feabc13-812b-4d5e-ac24-f8327ecb4d27\"}\nplt.figure(figsize=(6, 6))\nfor k in range(num_states):\n    plt.contour(XX, YY, np.exp(lls[:,k]).reshape(XX.shape), cmap=white_to_color_cmap(colors[k]))\n    plt.plot(data[true_states==k, 0], data[true_states==k, 1], 'o', mfc=colors[k], mec='none', ms=4)\n    \nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nplt.title(\"Observation Distributions\")\n\n\n# + [markdown] nbpresent={\"id\": \"a58c7a02-2777-4af8-982f-e279bd3bbeb6\"}\n# Below, we visualize each component of of the observation variable as a time series. The colors correspond to the latent state. The dotted lines represent the \"true\" values of the observation variable (the mean) while the solid lines are the actual observations sampled from the HMM.\n# -\n\n# # Cluster states from the original HMM\n#\n# The `merge_two_states` function below takes in a trained HMM, and indices of two states, s1 and s2. It outputs a new HMM where all states except for s1 and s2 are the same, along with the log-likelihood of the data under the new model.\n#\n# Here's how we merge two states: In the E-step of the EM algorithm, we obtain a T x K table, which has the probability of being in state K at time T for every time point. To merge state k1 and k2, we take the two columns of the table corresponding to these two states and sum them. From this, we get a new table which is K-1 x T. We then run an M-step as normal to get the best parameters for our new K-1 state model, and evaluate the log likelihood.\n#\n# **NOTE**: as written, the below function does not support inputs or masks, and it is limited to HMMs with stationary transitions. \n\ndef merge_two_states(hmm, s1, s2, datas, observations=\"gaussian\"):\n    \n    def collapse_and_sum_2d(arr, i, j, axis=0):\n        assert axis <= 1\n        out = arr.copy()\n        if axis == 0:\n            out[i,:] += out[j,:]\n            return np.delete(out, j, axis=0)\n        if axis == 1:\n            out[:, i] += out[:, j]\n            return np.delete(out, j, axis=1)\n        \n    K = hmm.K\n    D = hmm.D\n    assert K >= 2\n    assert s1 < K\n    assert s2 < K\n    assert s1 != s2\n    datas = datas if isinstance(datas, list) else [datas]\n    inputs, masks, tags = [None], [None], [None]\n    expectations = [hmm.expected_states(data, input, mask, tag)\n                            for data, input, mask, tag in zip(datas, inputs, masks, tags)]\n    \n    # Merge expectations for 2 states\n    expectations_new = []\n    for (Ez, Ezz, py) in expectations:\n        T_curr = Ez.shape[0]\n        \n        # Put merged expectations in first column\n        Ez_new = collapse_and_sum_2d(Ez, s1, s2, axis=1)\n        \n        # Now merge Ezz\n        # Ezz will have shape 1, K, K\n        # so we get rid of the first dimension then add it back.\n        Ezz_new = collapse_and_sum_2d(Ezz[0], s1, s2, axis=0)\n        Ezz_new = collapse_and_sum_2d(Ezz_new, s1, s2, axis=1)\n        Ezz_new = Ezz_new[None, :, :]\n        \n        expectations_new.append((Ez_new, Ezz_new, py))\n    \n    # Perform M-Step to get params for new hmm\n    new_hmm = ssm.HMM(K-1, D, observations=observations)\n    new_hmm.init_state_distn.m_step(expectations_new, datas, inputs, masks, tags)\n    new_hmm.transitions.m_step(expectations_new, datas, inputs, masks, tags)\n    new_hmm.observations.m_step(expectations_new, datas, inputs, masks, tags)\n    \n    # Evaluate log_likelihood\n    expectations = [new_hmm.expected_states(data, input, mask, tag)\n                    for data, input, mask, tag in zip(datas, inputs, masks, tags)]\n    new_ll = new_hmm.log_prior() + sum([ll for (_, _, ll) in expectations])\n    return new_ll, new_hmm\n        \n\n\ndef plot_hmm(hmm, data):\n    # Get the most likely state sequence\n    states = hmm.most_likely_states(data)\n\n    # Plot the observation distributions in the new model\n    lim = .85 * abs(data).max()\n    XX, YY = np.meshgrid(np.linspace(-lim, lim, 100), np.linspace(-lim, lim, 100))\n    grid = np.column_stack((XX.ravel(), YY.ravel()))\n    input = np.zeros((grid.shape[0], 0))\n    mask = np.ones_like(grid, dtype=bool)\n    tag = None\n    lls = hmm.observations.log_likelihoods(grid, input, mask, tag)\n\n    plt.figure(figsize=(6, 6))\n    for k in range(hmm.K):\n        plt.contour(XX, YY, np.exp(lls[:,k]).reshape(XX.shape), cmap=white_to_color_cmap(colors[k]))\n        plt.plot(data[states==k, 0], data[states==k, 1], 'o', mfc=colors[k], mec='none', ms=4)\n\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\")\n    plt.title(\"Observation Distributions in merged HMM\")\n    \n\n\n# +\nnew_ll, new_hmm = merge_two_states(true_hmm, 0, 3, data)\nprint(\"likelihood drop: \", new_ll - true_ll)\n\nplot_hmm(new_hmm, data)\n# -\n\n# Blue (0) and green (3) in the original model were not really similar, so we expected to see a big drop in likelihood in the merged model.  Let's do the same with the last two states, which we made similar by construction.\n\n# +\nnew_ll, new_hmm = merge_two_states(true_hmm, 4, 5, data)\nprint(\"likelihood drop: \", new_ll - true_ll)\n\nplot_hmm(new_hmm, data)\n\n\n# -\n\n# Good! Looks like the drop in likelihood is actually a little lower for the \"more similar\" states.\n\n# ## Make a pairwise similarity matrix\n#\n# We can use the log-likelihood drop when merging states as a proxy for state \"similarity.\" Two states which can be merged with minimal drop in likelihood might be considered similar.\n\ndef make_similarity_matrix(hmm, data):\n    num_states = hmm.K\n    init_ll = hmm.log_probability(data)\n    similarity = np.nan * np.ones((num_states, num_states))\n    merged_hmms = np.empty((num_states, num_states), dtype=object)\n    for s1 in range(num_states-1):\n        for s2 in range(s1+1, num_states):\n            merged_ll, merged_hmm = merge_two_states(hmm, s1, s2, data)\n            similarity[s1, s2] = merged_ll - init_ll\n            merged_hmms[s1, s2] = merged_hmm\n            \n    return similarity, merged_hmms\n\n\nsimilarity, new_hmms = make_similarity_matrix(true_hmm, data)\nim = plt.imshow(similarity)\nplt.ylabel(\"state 1\")\nplt.xlabel(\"state 2\")\nplt.title(\"similarity\")\nplt.colorbar()\n\n\n# These merges are perhaps a little counterintuitive at first.  In terms of likelihood drop, the first two states to be merged woudl be red (1) and yellow (2).  Their means more fairly different than those of purple and orange, but they have relatively large variance.  Let's see what happens when we do this recursively.\n\n# ## Hierarchical clustering by iteratively merging states\n# We start with a K state HMM, then merge possible pair of states k1 and k2. We can see which are the best two states to merge by checking the new log-likelihood. We then rinse and repeat for our new K-1 state HMM, tracking the log-likelihood as we go, until there is only 1 state left. After each merge, we can show the observation distribution and new similarity matrix.\n\ndef hierarchical_cluster(hmm, data, plot=True):\n    num_states = hmm.K\n    linkage = [None]\n    likelihood_drops = [0]\n    hmms = [hmm]\n    \n    if plot:\n        plot_hmm(hmm, data)\n    \n    for i in range(num_states - 1):\n        similarity, merged_hmms = make_similarity_matrix(hmms[-1], data)\n        \n        # Find the most similar states\n        s1, s2 = np.where(similarity == np.nanmax(similarity))\n        s1, s2 = s1[0], s2[0]\n        linkage.append((s1, s2))\n        likelihood_drops.append(similarity[s1, s2])\n        hmms.append(merged_hmms[s1, s2])\n        print(\"merging \", color_names[s1], \"and\", color_names[s2])\n        \n        if plot:\n            plt.figure()\n            im = plt.imshow(similarity)\n            plt.ylabel(\"state 1\")\n            plt.xlabel(\"state 2\")\n            plt.title(\"similarity\")\n            plt.colorbar()\n            \n            plt.figure()\n            plot_hmm(hmms[-1], data)\n    \n    return linkage, likelihood_drops, hmms\n\n\nlinkage, likelihood_drops, hmms = hierarchical_cluster(true_hmm, data)\n\n\n# ## Now plot the dendrogram using likelihood drop as similarity\n\n# +\ndef dendrogram(num_states, linkage, likelihood_drops):\n    plt.figure()\n    \n    def _plot_level(s1, s2, likelihood_drop, xs, offset):\n        new_offset = offset - likelihood_drop\n        for x in xs:\n            plt.plot([x, x], [offset, new_offset], '-k', lw=2)\n        plt.plot([xs[s1], xs[s2]], [new_offset, new_offset], '-k', lw=2)\n\n        new_xs = xs.copy()\n        new_xs[s1] = xs[s1] + (xs[s2] - xs[s1]) * npr.rand()\n        new_xs = np.concatenate([new_xs[:s2], new_xs[s2+1:]])\n        return new_xs, new_offset\n    \n    xs = np.arange(num_states, dtype=float)\n    offset = 0\n    for (s1, s2), drop in zip(linkage[1:], likelihood_drops[1:]):\n        xs, offset = _plot_level(s1, s2, drop, xs, offset)\n        \n    plt.xlabel(\"state\")\n    plt.ylabel(\"likelihood drop\")\n        \ndendrogram(true_hmm.K, linkage, likelihood_drops)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}